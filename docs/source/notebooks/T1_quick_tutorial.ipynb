{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1. Quick Tutorial\n",
    "## General process\n",
    "The process of correlation is composed of five steps:\n",
    "\n",
    "1. Loading data\n",
    "2. Processing data\n",
    "3. Loading data into a layer object\n",
    "4. Fixing distortion\n",
    "5. Performing the correlation\n",
    "\n",
    "In this tutorial, we'll guide you through these steps with a focus on a particular type of data: an EBSD mapping result of a plastic deformation field around a carbide particle in a nickel-based superalloy.\n",
    "\n",
    "## 1. Loading Data\n",
    "Loading data is an essential first step in the correlation process. The type of data to be loaded can vary greatly depending on the specific context or project you are working on. Here, we'll demonstrate how to load an example EBSD (Electron Backscatter Diffraction) file.\n",
    "\n",
    "In this case, we're going to use the genfromtxt method from the NumPy library to load the data into a structured array. Once loaded, we can visually examine the data by plotting it using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load EBSD data\n",
    "import numpy as np\n",
    "\n",
    "EBSD = np.genfromtxt(\n",
    "    \"./data/SiC_in_NiSA.ctf\", dtype=float, skip_header=15, delimiter=\"\\t\", names=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon successful loading, the data can be examined. It is crucial to understand the nature of the data, its structure and its attributes, as these factors could significantly affect the subsequent steps in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check EBSD data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, constrained_layout=True)\n",
    "\n",
    "axs[0].scatter(EBSD[\"X\"], EBSD[\"Y\"], c=EBSD[\"BC\"], s=2, cmap=\"gray\")\n",
    "axs[0].set_title(\"BC\")\n",
    "\n",
    "axs[1].scatter(EBSD[\"X\"], EBSD[\"Y\"], c=EBSD[\"Phase\"], s=2, cmap=\"cividis\", vmax=4)\n",
    "axs[1].set_title(\"Phase\")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_aspect(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the resulting structured array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EBSD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data Into the Layer\n",
    "Layer object represent a layer of measurement for a material, such as EBSD data. In this tutorial, we are going to discuss how to load EBSD data into a `Layer` object. More comprehensive details about constructing and manipulating `Layer` object will be covered in a later tutorials.\n",
    "\n",
    "### Direct construction of the Layer object\n",
    "We can create a `Layer` object and load our data into it directly. Here is a example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into the layer\n",
    "from pyxc.core.layer import Layer\n",
    "from pyxc.core.processor.arrays import column_parser\n",
    "from pyxc.core.container import Container2D\n",
    "from pyxc.core.loader import ImageLoader, XYDLoader\n",
    "from pyxc.transform.homography import Homography\n",
    "\n",
    "layer_ebsd = Layer(\n",
    "    data=column_parser(EBSD, format_string=\"dxydddddddd\"),\n",
    "    container=Container2D,\n",
    "    dataloader=XYDLoader,\n",
    "    transformer=Homography,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with geometric distortions\n",
    "Given that we've opted for the Homography transformation method, we'll employ a 3x3 transformation matrix. This matrix can be constructed using various libraries, including OpenCV. For now, let's proceed under the assumption that we already possess the necessary transformation matrix to rectify the distortions in our layer.\n",
    "\n",
    "Don't forget explicitly apply the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformation to the layer\n",
    "transformation_matrix = np.array(\n",
    "    [\n",
    "        [1, 0.1, 10],\n",
    "        [0, 1.0, 30],\n",
    "        [0, 0.0, 1],\n",
    "    ]\n",
    ")\n",
    "\n",
    "layer_ebsd.set_transformation_matrix(transformation_matrix)\n",
    "layer_ebsd.apply_transformation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly check whether the transformation is correctly applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check EBSD data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "\n",
    "ax.scatter(\n",
    "    layer_ebsd.container[\"x_raw\"],\n",
    "    layer_ebsd.container[\"y_raw\"],\n",
    "    c=layer_ebsd.container[\"BC\"],\n",
    "    s=2,\n",
    "    cmap=\"gray\",\n",
    ")\n",
    "ax.text(0, 0, \"Before transformation\")\n",
    "\n",
    "ax.scatter(\n",
    "    layer_ebsd.container[\"x\"],\n",
    "    layer_ebsd.container[\"y\"],\n",
    "    c=layer_ebsd.container[\"BC\"],\n",
    "    s=2,\n",
    "    cmap=\"magma\",\n",
    ")\n",
    "ax.text(10, 30, \"Transformed\")\n",
    "\n",
    "ax.set_aspect(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying by (X, Y) Coordinates\n",
    "\n",
    "It is possible to retrieve data for a specific location by querying with (X, Y) coordinates. Two parameters, 'cut-off' and 'output_number', play crucial roles in this process.\n",
    "\n",
    "The 'cut-off' parameter determines the maximum Euclidean distance from the query point $(X_{\\text{query}}, Y_{\\text{query}})$ to a nearby data point $(X_{\\text{data}}, Y_{\\text{data}})$ beyond which the data point will be disregarded.\n",
    "\n",
    "The 'output_number' parameter specifies the maximum number of closest valid data points to the query point that will be returned. For instance, if there are 10 valid data points within the cut-off circle and 'output_number' is set to 5, only the nearest 5 points will be returned.\n",
    "\n",
    "Let's explore the query method. If no valid data points are found near the query point, a NaN (Not a Number) value will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query data (after transformation there is no point in 10, 10 coordinate\n",
    "query_invalid = layer_ebsd.query(10, 10, cutoff=5, output_number=10)\n",
    "\n",
    "query_invalid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case that valid data points exist within the 'cut-off' distance from the query point, the query method will successfully return the data corresponding to these points, up to the limit set by 'output_number'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_valid = layer_ebsd.query(30, 40, cutoff=5, output_number=10)\n",
    "\n",
    "query_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Reducer` object allows for the execution of statistical operations on your data. This capability is especially useful when you have multiple data points, i.e., when 'output_number' is more than 1.\n",
    "\n",
    "The `Reducer` object's format is defined as `Iterable[Tuple[Callable, Iterable[Column Names]]]`. In this structure, `Callable` refers to the statistical function to be applied, while `Iterable[Column Names]` is a list of the columns on which this function will be applied.\n",
    "\n",
    "When you use a `Reducer` object, the resulting query columns will be altered. The new format of each column will be `ColumnName_CallableName`, where `CallableName` is the name of the statistical function applied and `ColumnName` is the original column name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyxc.core.processor.reducer import Reducer\n",
    "\n",
    "reducer_obj = Reducer([(np.mean, [\"BS\", \"Phase\"]), (np.std, [\"BS\", \"Phase\"])])\n",
    "query_valid_with_reducer = layer_ebsd.query(\n",
    "    30, 40, cutoff=5, output_number=10, reducer=reducer_obj\n",
    ")\n",
    "\n",
    "query_valid_with_reducer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Also, when the 'output_number' is set to more than 1 (to use `execute_query` method), it becomes necessary to supply a `Reducer` object. This is because when multiple rows of data are returned by each query, there is ambiguity about how to consolidate these results into a single array. To resolve this, the `Reducer` object is employed to reduce these multiple rows of data into a single entry, thus ensuring a consistent data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = np.meshgrid(np.arange(20, 30, 1), np.arange(35, 45, 1))\n",
    "xs, ys = xs.flatten(), ys.flatten()\n",
    "\n",
    "bulk_query = layer_ebsd.execute_queries(\n",
    "    xs, ys, cutoff=2, output_number=2, reducer=reducer_obj\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, it is feasible to perform multiple queries simultaneously for added convenience. These queries are executed in parallel to enhance efficiency.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Warning\n",
    "\n",
    "See the code below very carefully. There is no guarantee that all points that you have provided yield a correlation result. If the points are too far away from the data point (beyond the cut-off distance), you will not get the result. You will be required to filter out the points that are not hit by using the `query_index` column.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = np.meshgrid(np.arange(20, 30, 1), np.arange(35, 45, 1))\n",
    "xs, ys = xs.flatten(), ys.flatten()\n",
    "\n",
    "bulk_query = layer_ebsd.execute_queries(xs, ys, cutoff=2, output_number=1)\n",
    "\n",
    "xs_filtered = xs[bulk_query[\"query_index\"]]\n",
    "ys_filtered = ys[bulk_query[\"query_index\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the query result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, sharex=\"all\", sharey=\"all\")\n",
    "\n",
    "ax[0].scatter(*layer_ebsd.get_xy(), c=layer_ebsd.container[\"BC\"], cmap=\"magma\", s=1)\n",
    "ax[0].scatter(xs, ys, c=\"#ffffff\", marker=\"+\", s=20)\n",
    "ax[0].set_title(\"Layer & query point\")\n",
    "\n",
    "ax[1].scatter(\n",
    "    bulk_query[\"x-coordinates\"], bulk_query[\"y-coordinates\"], c=bulk_query[\"Phase\"], s=1\n",
    ")\n",
    "ax[1].set_title(\"Query result (Phase)\")\n",
    "\n",
    "for a in ax:\n",
    "    a.set_aspect(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
